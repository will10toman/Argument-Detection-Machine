{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01374f3a",
   "metadata": {},
   "source": [
    "## Scraping Wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e0a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_random_wikipedia_article():\n",
    "    \"\"\"Scrape a random Wikipedia article\"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Get the article title for logging\n",
    "    title = soup.find('h1', {'id': 'firstHeading'})\n",
    "    title_text = title.get_text() if title else \"Unknown\"\n",
    "    \n",
    "    sentences = []\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        text = p.get_text().strip()\n",
    "        if len(text) > 50:\n",
    "            sents = text.split('.')\n",
    "            for sent in sents:\n",
    "                sent = sent.strip()\n",
    "                if 20 < len(sent) < 250:\n",
    "                    sentences.append(sent)\n",
    "    \n",
    "    return sentences, title_text\n",
    "\n",
    "print(\"Scraping random Wikipedia articles...\")\n",
    "print(\"Target: 25,000 sentences\\n\")\n",
    "\n",
    "all_sentences = []\n",
    "articles_processed = 0\n",
    "articles_with_content = 0\n",
    "\n",
    "while len(all_sentences) < 25000:\n",
    "    try:\n",
    "        sentences, title = scrape_random_wikipedia_article()\n",
    "        articles_processed += 1\n",
    "        \n",
    "        if len(sentences) > 0:\n",
    "            all_sentences.extend(sentences)\n",
    "            articles_with_content += 1\n",
    "            \n",
    "            if articles_processed % 10 == 0:\n",
    "                print(f\"Articles processed: {articles_processed} | \"\n",
    "                      f\"With content: {articles_with_content} | \"\n",
    "                      f\"Total sentences: {len(all_sentences)}\")\n",
    "        \n",
    "        time.sleep(0.5)  # Be polite to Wikipedia\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "# Save first 25000\n",
    "df = pd.DataFrame({\n",
    "    'sentence': all_sentences[:25000],\n",
    "    'tag': 'non_info'\n",
    "})\n",
    "\n",
    "print(f\"\\n✓ Collection complete!\")\n",
    "print(f\"✓ Articles processed: {articles_processed}\")\n",
    "print(f\"✓ Articles with content: {articles_with_content}\")\n",
    "print(f\"✓ Total sentences collected: {len(df)}\")\n",
    "\n",
    "df.to_csv('non_info_data.csv', index=False)\n",
    "print(\"✓ Saved to non_info_data.csv\")\n",
    "\n",
    "# Show some samples\n",
    "print(\"\\nSample sentences:\")\n",
    "for sent in df['sentence'].sample(5):\n",
    "    print(f\"  - {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_info_sentences(df):\n",
    "    \"\"\"\n",
    "    Clean non-info sentences with specific rules:\n",
    "    1. Remove sentences ending in \":\"\n",
    "    2. Remove unicode characters from sentences (keep the sentence)\n",
    "    3. Remove sentences that are coordinates\n",
    "    4. Remove sentences with multiple consecutive special characters\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting size: {len(df)}\")\n",
    "    \n",
    "    # 1. Remove sentences ending with \":\"\n",
    "    df = df[~df['sentence'].str.endswith(':')].copy()\n",
    "    print(f\"After removing sentences ending in ':': {len(df)}\")\n",
    "    \n",
    "    # 2. Remove unicode characters from sentences (but keep the sentences)\n",
    "    df['sentence'] = df['sentence'].str.replace('\\ufeff', '', regex=False)\n",
    "    df['sentence'] = df['sentence'].str.replace('\\xa0', ' ', regex=False)\n",
    "    df['sentence'] = df['sentence'].str.replace('\\u200b', '', regex=False)\n",
    "    df['sentence'] = df['sentence'].str.replace('\\u200e', '', regex=False)\n",
    "    df['sentence'] = df['sentence'].str.replace('\\u200f', '', regex=False)\n",
    "    print(f\"Unicode characters removed from sentences\")\n",
    "    \n",
    "    # 3. Remove sentences that are coordinates (like \"21°23′35″N 92°00′02″E\")\n",
    "    coordinate_pattern = r'\\d+°\\d+′\\d+″'\n",
    "    df = df[~df['sentence'].str.contains(coordinate_pattern, regex=True, na=False)].copy()\n",
    "    print(f\"After removing coordinate sentences: {len(df)}\")\n",
    "    \n",
    "    # 4. Remove sentences with 4+ consecutive special characters\n",
    "    multiple_special_pattern = r'[^\\w\\s]{4,}'\n",
    "    df = df[~df['sentence'].str.contains(multiple_special_pattern, regex=True, na=False)].copy()\n",
    "    print(f\"After removing sentences with multiple special characters: {len(df)}\")\n",
    "    \n",
    "    # Clean up any extra whitespace created by unicode removal\n",
    "    df['sentence'] = df['sentence'].str.strip()\n",
    "    df['sentence'] = df['sentence'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    print(f\"\\nFinal size: {len(df)}\")\n",
    "    print(f\"Total removed: {len(df) - len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Use it\n",
    "df = pd.read_csv('non_info_data.csv')\n",
    "df_clean = clean_non_info_sentences(df)\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n=== Sample cleaned sentences ===\")\n",
    "for sent in df_clean['sentence'].sample(10):\n",
    "    print(f\"- {sent}\")\n",
    "\n",
    "# Save\n",
    "df_clean.to_csv('non_info_data_clean.csv', index=False)\n",
    "print(f\"\\n✓ Saved to non_info_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4edfffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_info_clean = pd.read_csv('non_info_data_clean.csv')  # has 'sentence' and 'tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c92548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('argumentdetection6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862c2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_info_clean = non_info_clean.rename(columns={'tag': 'category'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5213e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([data, non_info_clean], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8e9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evidence    833983\n",
       "non_info     25221\n",
       "claim         1662\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc298e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('argumentdetection7.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
